{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re  # regular expressions (for playing with the text)\n",
    "from gensim.parsing.preprocessing import STOPWORDS # common english \"stop words\" -- a, the, etc.\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import nltk.data\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from csv import reader\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import csv\n",
    "from nltk import bigrams \n",
    "from nltk import trigrams\n",
    "import datetime, pytz, time\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import string\n",
    "import html\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from textblob import TextBlob\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "path_to_json = 'reddit_data/'\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    " \n",
    "def run_cmd(args_list):\n",
    "        print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "        proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        s_output, s_err = proc.communicate()\n",
    "        s_return =  proc.returncode\n",
    "        return s_return, s_output, s_err \n",
    "\n",
    "def apos_words(text):\n",
    "    sentence=[]\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            sentence.append(apos_dict[word.lower()])\n",
    "        except:\n",
    "            sentence.append(' '.join(word.split(\"'\")))\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "def tweet_text_preprocess(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text=re.sub(r'@\\w+',r'',re.sub(r'rt @\\w+:',r'',re.sub(r'\\\\x[0-9A-Fa-f]+',r'', re.sub(r'\\\\n',r'',text)))) #,r'(?:RT @[\\w_:]+) ',r''\n",
    "    text=re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%]+\\.[\\w/\\-?=%]+',r'',apos_words(text))\n",
    "    text = ' '.join(re.findall(\"[\\S]+\", html.unescape(text))).replace(\"#\",\" \").replace(\"\\n\",'').replace(\"\\t\",'').replace(\"\\r\",'').replace(\"\\f\",'').replace(\"\\v\",'')\n",
    "    return ' '.join(text.translate(translator).split()).lower()\n",
    "\n",
    "\n",
    "\n",
    "def Zone_conversion(timestamp):\n",
    "    \n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('EDT')\n",
    "    utc = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    central = utc.astimezone(to_zone)\n",
    "    return (str(central)[:-6])\n",
    "\n",
    "\n",
    "def remove_emoji(doc):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', doc)\n",
    "\n",
    "\n",
    "global_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_name_and_https(corp):\n",
    "    for doc in corp:\n",
    "        yield  re.sub(r'\\//t.','', re.sub(r'https.*\\b\\S',' ' , re.sub(r'\\@\\w+', ' ', doc)))\n",
    "        \n",
    "        \n",
    "def remove_small_words(corp):\n",
    "    for doc in corp:\n",
    "        yield  re.sub(r'\\brt|\\bRT','',doc)\n",
    "\n",
    "\n",
    "def read_corpus_basic(corp):\n",
    "    for doc in corp:\n",
    "        yield [x for x in gensim.utils.simple_preprocess(doc, deacc=True)]\n",
    "\n",
    "def read_corpus_with_stemming_and_SW_removal(corp):\n",
    "    for doc in corp:\n",
    "        yield [global_stemmer.stem(x) for x in gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "                   if x.lower() not in STOPWORDS]\n",
    "        \n",
    "def read_corpus_with_lemmitizer_and_SW_removal(corp):\n",
    "    for doc in corp:\n",
    "        yield [wordnet_lemmatizer.lemmatize(x) for x in gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "                   if x.lower() not in STOPWORDS]\n",
    "        \n",
    "\n",
    "def get_senti_info(text,senti):\n",
    "    pos_count = neg_count = 0\n",
    "    if senti == \"score\":\n",
    "        return round(sid.polarity_scores(text)['compound'],1)\n",
    "    else:\n",
    "        if senti == \"pos\":\n",
    "            for w in text.split(\" \"):\n",
    "                if sid.polarity_scores(w)['pos'] == 1:\n",
    "                    pos_count+=1\n",
    "        elif senti == \"neg\":\n",
    "            for w in text.split(\" \"):\n",
    "                if sid.polarity_scores(w)['neg'] == 1:\n",
    "                    neg_count+=1\n",
    "        else: pass\n",
    "        return pos_count if senti == \"pos\" else neg_count        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global apos_dict\n",
    "apos_dict = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"can not\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"I'd\" : \"I would\",\n",
    "\"I'll\" : \"I will\",\n",
    "\"I'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"y'all\": \"you all\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\"we will\",\n",
    "\"didn't\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncs=[]\n",
    "for i in punctuation:\n",
    "    puncs.append(str(i))\n",
    "\n",
    "list1 = ['RT','rt', 'na', 'fx', 'th', 'http', 'lt', 'v', 'hd', 'pm', 'im', 'ga', 'amp', 'got', 'jf', 'ni'\n",
    "         'db', 'way', 'ge', 'nv', 'ol', 'bd', 'ba', 'nf', 'se', 'dj', 'gt', 'georgia', 'hi', 'href', 'new']\n",
    "List2 = ['#atlanta', u'#atlanta',  '#atlantas', u'#atlantas', '#ga ', '\"', '#ga']\n",
    "List2.extend(['this','that','the','might','have','been','from','ve',\n",
    "                'but','they','will','has','having','had','how','went'\n",
    "                'were','why','and','still','his','her','was','its','per','cent',\n",
    "                'a','able','about','across','after','all','almost','also','am','among',\n",
    "                'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "                'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "                'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "                'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "                'like','likely','may','me','might','most','must','my','neither','nor','ll',\n",
    "                'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "                'say','says','she','should','since','so','some','than','that','the','their','dez',\n",
    "                'them','then','there','these','they','this','tis','to','too','twas','us','nufx','cqw',\n",
    "                'wants','was','we','were','what','when','where','which','while','who','vykvyjcn',\n",
    "                'whom','why','will','with','would','yet','you','your','ve','re','rt', 'retweet', '#fuckem', '#fuck',\n",
    "                'fuck', 'ya', 'yall', 'yay', 'youre', 'youve', 'ass','factbox', 'com', '&lt', 'th',\n",
    "                'retweeting', 'dick', 'fuckin', 'shit', 'via', 'fucking', 'shocker', 'wtf', 'hey', 'ooh', 'rt&amp', '&amp',\n",
    "                '#retweet', 'retweet', 'goooooooooo', 'hellooo', 'gooo', 'fucks', 'fucka', 'bitch', 'wey', 'sooo', \n",
    "              'helloooooo', 'lol', 'smfh', 'bitch', 'atlanta', 'black', 'nigga', 'georgia', 'forward', 'today','tho','qjlgjezq'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "header= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hadoop fs -get /data/atl_sprint_2018/reddit_archive/reddit20190208T0000.json reddit_data/\n",
      "name 'header' is not defined\n"
     ]
    }
   ],
   "source": [
    "#Hadoop Command to get data to local repository(Daily Files)\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        a=datetime.now()\n",
    "        \n",
    "        yesterday = date.today() - timedelta(1)\n",
    "        \n",
    "        dates=str(a.year)+str(a.month).zfill(2)+str(a.day).zfill(2)\n",
    "        #date1=str(a.year)+'-'+str(a.month).zfill(2)+'-'+str(a.day-1).zfill(2)\n",
    "        \n",
    "        yesterday_date=str(yesterday.strftime('%m-%d-%Y'))\n",
    "\n",
    "        #print(yesterday_date)\n",
    "        (ret, out, err)= run_cmd(['hadoop', 'fs', '-get', '/data/atl_sprint_2018/reddit_archive/reddit%sT0000.json'\n",
    "                            %(dates), 'reddit_data/'])\n",
    "\n",
    "        reddit_temp=[]\n",
    "        Reddit_Data = pd.DataFrame()\n",
    "        time.sleep(30) \n",
    "        with open('%sreddit%sT0000.json' %(path_to_json,dates), 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    reddit = json.loads(line)\n",
    "                    reddit_temp.append(reddit)\n",
    "                except:\n",
    "                    continue \n",
    "        #Reddit_Data = pd.read_csv('%sreddit%sT0000.csv' %(path_to_json,dates), encoding='utf-8')\n",
    "        Reddit_Data=Reddit_Data.from_records(reddit_temp)\n",
    "        Reddit_Data['TimeZone_old'] = Reddit_Data[['Date', 'Time']].apply(lambda x: ' '.join(x), axis=1)\n",
    "        Reddit_Data['TimeZone_new'] = [ Zone_conversion(i) for i in Reddit_Data.TimeZone_old]\n",
    "        Reddit_Data['Date_new'] = [ datetime.strptime(str(i)[0:10],\"%Y-%m-%d\") for i in Reddit_Data.TimeZone_new]\n",
    "        ##new code below\n",
    "        #-------------------\n",
    "        #Reddit_Data['Date_new'] = yesterday_date\n",
    "        #--------------------------------\n",
    "        Reddit_Data['Time_new'] = [ str(i)[11:] for i in Reddit_Data.TimeZone_new]\n",
    "        Reddit_Data['CleanedText'] = [tweet_text_preprocess(str(i)) for i in Reddit_Data.Text]\n",
    "        Reddit_Data['polarity'] = [round(sid.polarity_scores(i)['compound'],1) for i in Reddit_Data.CleanedText]\n",
    "        #print(Reddit_Data)\n",
    "        #d = a - timedelta(days=1)\n",
    "        mode = 'w' if header else 'a'\n",
    "        Reddit_Data[Reddit_Data.Date_new==yesterday_date].groupby([\"Date_new\"]).Text.count().reset_index(name=\"Volume\").to_csv(\"Reddit_Output/Reddit_Daily_Volume.csv\",sep=\",\",header=header,mode=mode,index=False)\n",
    "        sent_temp=[]\n",
    "        for i in range(0,len(Reddit_Data)):\n",
    "            if Reddit_Data['polarity'][i]<0.0:\n",
    "                sent_temp.append('Negative')\n",
    "            elif Reddit_Data['polarity'][i]>0.0:\n",
    "                sent_temp.append('Positive')\n",
    "            else:\n",
    "                sent_temp.append('Neutral')\n",
    "        Reddit_Data['Sentiment']=sent_temp\n",
    "        Reddit_Data[Reddit_Data.Date_new==yesterday_date].groupby([\"Date_new\",\"Sentiment\"]).Text.count().reset_index(name=\"Volume\").to_csv(\"Reddit_Output/Reddit_Overall_Sentiment.csv\",sep=\",\",header=header,mode=mode,index=False)\n",
    "        header=False\n",
    "    \n",
    "        temp=[]\n",
    "        corpus=[]\n",
    "        list_corp=list(Reddit_Data[Reddit_Data.Date_new==yesterday_date]['CleanedText'])\n",
    "        for i in range(len(Reddit_Data[Reddit_Data.Date_new==yesterday_date]['CleanedText'])):\n",
    "            temp=apos_words(remove_emoji(str(list_corp[i])))\n",
    "            corpus.append(temp)\n",
    "\n",
    "        corpus2 =  list(read_corpus_with_lemmitizer_and_SW_removal(\n",
    "                    list( remove_small_words(\n",
    "                        list(remove_name_and_https(\n",
    "                            list(corpus)))))))\n",
    "\n",
    "        #Selecting 2 letter words for exclusion    \n",
    "        text_check=[x for doc in corpus2 for x in doc]\n",
    "        two_letters = list(set([word for word in text_check if len(word) <= 2 and word not in stopwords.words('english')]))    \n",
    "        stoplist = list1+puncs+two_letters\n",
    "\n",
    "        text_token = [[x for x in doc if x not in stoplist] for doc in corpus2]\n",
    "        flat_list = [item for sublist in text_token for item in sublist]\n",
    "        word_list = nltk.FreqDist(flat_list)\n",
    "\n",
    "\n",
    "        master_word_list = []\n",
    "        for k, v in word_list.items():\n",
    "            temp =[]\n",
    "            temp.append(k)\n",
    "            temp.append(v)\n",
    "            master_word_list.append(temp)\n",
    "\n",
    "        headers = ['Word','Freq']\n",
    "        final_df = pd.DataFrame(master_word_list, columns=headers)\n",
    "\n",
    "        final_df.to_csv(\"Reddit_Output/Reddit_WordCloud.csv\",sep=\",\",header=True,mode='w')\n",
    "        \n",
    "###################Clearing the Home Folder after every 7 days######################       \n",
    "        file_list=[]\n",
    "        for the_file in os.listdir(path_to_json):\n",
    "            file_list.append(the_file)\n",
    "        try:\n",
    "            if len(file_list)==7:\n",
    "                for f in file_list:\n",
    "                    os.remove(os.path.join(path_to_json,f))       \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        time.sleep(86400)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        time.sleep(3600)\n",
    "        continue\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
